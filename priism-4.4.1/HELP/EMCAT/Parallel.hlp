KEYWORD
    CPUParallel GPUParallel

DESCRIPTION
    To speed up some EM tomography calculations, you
    can run the calculations on a cluster of machines,
    on a single machine which has multiple processors,
    or on one or more machines with NVIDIA graphics
    cards. The weighted backprojection (EWBP), iterative
    reconstruction (TAPIR), and GPU-accelerated
    reconstruction in the alignment and reconstruction
    process can be spread across multiple processors as
    can parts of the iterative alignment procedures
    (refine alignment, refine alignment with GPU
    acceleration, iterative alignment and reconstruction,
    and iterative alignment and reconstruction with GPU
    acceleration).

    By default, the parallel execution settings are
    derived from the configuration of the queue or
    machine where the reconstruction will be done (you
    can control that destination with the "Options"
    button at the bottom of the main dialog).  To view
    or change the settings which apply to running
    calculations in parallel on multiple CPUs (EWBP,
    TAPIR>, refine alignment, or iterative alignment
    and reconstruction), turn off the the "default"
    toggle button next to the "Parallel execution
    settings..." button if it is not already off and
    then press the "Parallel execution settings..."
    button to bring up the dialog for the parallel
    execution parameters.  To view or change the
    settings which affect calculations run on GPUs
    (GPURECON, refine alignment (GPU), and iterative
    alignment and reconstruction (GPU)), turn off
    the "default" toggle button next to the
    "GPU execution settings..." button if it is not
    already off and then press the "GPU execution
    settings..." to bring up the dialog for the GPU
    execution parameters.

    If you find yourself changing the default
    parameters over and over again to the same set
    of values, you should set up the default
    configuration to avoid those changes.  You
    should also set up a default configuration if
    you want to integrate job submission with a
    queueing system.  The queue topic of
    BatchRegion.hlp describes how to configure the
    defaults.

CPUParallel
    Adjusting the parallel execution parameters for
    use on a single machine with multiple processors
    (or processor cores) is straightforward:

    1) Turn on the "Parallel" toggle button to enable
       parallel execution if it is not already on.

    2) Adjust the directory displayed in the
       "Temporary directory" field if necessary.  This
       is where temporary files will be placed.  If
       the directory does not already exist it will
       be created for you.  For the Iterative
       Alignment + Reconstruction and Refine Alignment
       processes, the "Temporary directory" field is
       in the main dialog rather than parallel
       execution dialog since the temporary directory
       is used regardless of whether or not the
       reconstruction is done in parallel.

    3) Select "single host" from the menu next to the
       button labeled "Parallel dispatch".

    4) Enter the number of processors to use in the
       "Number of processors" field.

    Handling a cluster is more involved.  Your cluster
    must first satisfy three requirements to be usable
    for the parallel execution of the EM reconstructions:

    1) There must be at least one filesystem that is
       visible to the machine starting the parallel job
       and all the machines in the cluster to which it
       distributes the work since the directory where
       temporary files are placed must be visible to
       all of those machines.  The input files and
       final output files only need to be in locations
       visible to the machine starting the parallel
       job.

    2) To distribute the work to the machines in the
       cluster, the parallel execution mode either
       relies on OpenMPI (version 1.3 or later;
       available from http://www.open-mpi.org ) or
       combines a list of machines to access with
       a remote login command.  The former method
       is the recommended one:  it is more robust,
       works with many queueing environments with
       little extra work, and the configuration
       necessary is similar to what is needed for
       using a list of machines and a remote login
       command.  The installation and configuration
       of OpenMPI is beyond the scope of this
       document.  Consult its installation
       instructions for more details.  Most of the
       configuration necessary for the second
       method is to set up a command that functions
       like rsh and allows access to the cluster
       nodes.  In particular, that command must
       satisfy the hese requirements:

       A) Accept the syntax,

              command host command_or_commands_to_execute &

          , to run command_to_commands_to_execute
          on host. 

       B) Does not require further intervention on
          your part (i.e. entering  password) to
          login and execute a command on a machine
          in the cluster.

       C) Will allow the commands to be run in the
          background when invoked with an option
          or options (typically -n) between the
          command's name and the host's name.
          When invoked without that option, must
          pass the standard input from the local
          machine to the other host.

       Choices for how to configure that with ssh
       would be to use ssh as the name of the
       command (as an optimization we use ssh -x
       so that X11 forwarding is disabled since
       that can introduce delays on at least some
       systems), -n as the background option, and
       to either use ssh-agent to supply the
       password, set up the keys so a password is
       not required (with OpenSSH this is done
       with an authorized_keys file), or use
       host-based authentication (with OpenSSH
       this uses .shosts or .rhosts (or the
       system-wide equivalents) and typically
       requires modifying the sshd configuration
       as well).   With rsh, you would use rsh
       as the name of the command, -n as the
       background option, and configure .rhosts
       (or the system-wide equivalent) to allow
       you to login to the cluster's machines
       without a password.  You must also
       construct a file which lists the host
       names of the machines in your cluster.
       The format of the list is the same as
       is used by MPICH for its machines file:
       one computer host name per line (but you
       can have comments, which begin with a
       #, and lines containing only white space)
       and to indicate machines with multiple
       processors or processor cores you can
       either list the host name multiple times
       or enter the host name followed immediately
       by a colon and the number of processors
       (for example, my4way.ucsf.edu:4).
       HTML/machines.txt in the Priism directories
       is an example of a list appropriate for six
       dual processor systems named compute-0-0,
       compute-0-1, ..., and compute-0-5.  If your
       cluster has been configured with MPICH, you
       likely already have a file listing the names
       of the machines in the share directory where
       MPICH is installed or may have a queueing
       system that automatically generates the list
       of machines in the appropriate format for
       each job.

    4) Priism will have to be installed on the
       machines in the cluster.  It could be
       installed locally or in a shared location
       visible to all the machines.  It is
       easiest if the path to Priism is the same
       on all the machines and on the machine
       where you run the front-ends to the
       reconstruction software.  If the path
       is not the same, you should configure
       the IVE_ENV_SETUP environment variable
       on the machine where you run the
       front-ends.  That environment variable
       is expected to hold a command that will
       correctly establish the Priism environment
       when the command is executed on a machine
       in the cluster.  For a little more
       information about the IVE_ENV_SETUP
       variable, look at the Priism_setup or
       Priism_setup.sh files in the top-level
       directory of the Priism distribution.

    Once you have done the preliminary work and
    want to run a reconstruction, make the
    following adjustments to the front-end
    controls in order to perform the
    reconstruction on the cluster:

    1) You will make your life easier if all
       file and directory file names you specify
       as parameters are absolute names (i.e.
       begin with /).  Some exceptions to that
       are the names of commands that are in the
       standard path or the name of the machine
       list which can be a relative in some cases.

    2) Turn on the "Parallel" toggle button to
       enable parallel execution if it is not
       already on.

    3) Adjust the directory displayed in the
       "Temporary directory" field if necessary.
       This is where temporary files will be
       placed.  If the directory does not already
       exist it will be created for you.  For the
       iterative alignment processes, the
       "Temporary directory" field is in the main
       dialog rather than parallel execution dialog
       since the temporary directory is used
       regardless of whether or not the
       reconstruction is done in parallel.

    4) If you use OpenMPI, select "OpenMPI mpiexec"
       from the menu next to the button labeled
       "Parallel dispatch".  Then enter the path to
       OpenMPI's mpiexec or mpirun and any necessary
       options in the field labeled "OpenMPI mpiexec
       path".  As an example, we have OpenMPI's
       mpiexec installed in /software/openmpi-1.6/bin
       but it is not in the standard path.  We will
       run jobs from a queueing system (Grid Engine),
       and OpenMPI knows how to determine the
       requested nodes for a job from that queueing
       system without any additional options.
       Therefore, we enter

           /software/openmpi-1.6/bin/mpiexec

       in "OpenMPI mpiexec path".

    5) If you use a machine list and a remote login
       command, select "MPICH-style machines file"
       from the next to the button labeled "Parallel
       dispatch".  The other changes you may need to
       make are:

       A) Enter the name of the file which lists
          the machine names in the field next to
          the "Machine list" button or press the
          "Machine list"  button to open a file
          browser for selecting the file.

       B) Enter the command for logging into a
          cluster machine without a password
          and running a command in the "rsh
          command" field.  The common choices for
          such a command are "ssh -x" (the default),
          "ssh", and "rsh".  If necessary, change
          the value (it defaults to be -n which is
          appropriate for ssh or rsh) in the
          "Background option" field to be the
          option or options necessary to run the
          command shown in the "rsh command"
          field in the background.

       C) You should normally leave the "Check
          command" field blank.  That field
          specifies a command to run immediately
          before remotely logging in to a compute
          node to a launch a task.  You might
          use that field to work around problems
          with starting tasks on the compute nodes.
          For instance, some old clusters of ours
          had intermittent problems with compute
          nodes failing to mount NFS directories
          from the head node; specifying

              \$rshcmd \"\$machine\" ls \"\$IVE_BASE\" >/dev/null 2>&1

          as the check command was a way of working
          around the problem.  As shown in that example,
          you can use shell variables in the check command:
          \$rshcmd expands to the remote login command
          to use and \$machine expands to the name of the
          compute node.

    6) With either OpenMPI or the machines list, you may
       specify a number of processors to use.  The value
       that you give can have two effects.  Firstly, it
       can influence the number of processors requested
       if you are submitting jobs to a queue.  That
       requires a customized configuration, see the
       queue topic of BatchRegion.hlp if you want to
       know more, and the selection of the queue to use
       from the "Options..." button in the main dialog.
       Secondly, the application software either passes
       the specified number of processors to the mpiexec
       command or uses that number to control the
       processors extracted from the machines file.
       With a custom queue configuration, the second
       effect may be disabled so that the requested
       number of processors only directly affects the
       request to the queueing software.

    Parallel reconstructions are performed by the
    ewbp_parallel and tapir_parallel command-line
    applications.  Parallel alignment of two tilt
    series is performed by the align_2d_pi_parallel
    command-line application.  Those applications
    accept the same arguments as their non-parallel
    counterparts (ewbp, tapir, and align_2d_pi
    respectively) and use the following options to
    control parallel execution:

    -bkgopt=option
        Causes the application to use option with
        the command specified with -rshcmd= when
        that command is to be run in the background.
        By default, the application uses "-n" as
        the option.

    -checkcmd=cmd
        If a list of machines has been supplied
        with -machinefile=, specifies a command
        to execute just before submitting a task
        to a compute node.  By default, nothing
        is executed.  This option is not affected
        by the -no_nodecheck option.  You can use
        shell variables in the command.  For
        instance, $machine will expand to the
        name of the compute node and $rshcmd will
        expand to the remote login command to use.
        In most cases (i.e. when you run
        ewbp_parallel, tapir_parallel, or
        align_2d_pi_parallel from within a shell),
        the dollar signs (and any other characters
        special to the shell) in the command will
        need to be escaped with a backslash.

    -machinefile=file
        If -ompiexec= is not used or specifies an
        empty file name and file is not an empty
        file name, causes the application to read
        the names of the hosts to use from file
        where file has the format of an MPICH
        machines file.  If file is an empty file
        name or you do not use -machinefile=,
        the parallel job will be executed through
        OpenMPI, if -ompiexec= specifies a
        non-empty value, or by running multiple
        processes on the host where the parallel
        script runs.

    -no_nodecheck
        If running under OpenMPI (the options
        include -ompiexec= with a non-empty value)
        or with a list of machines that has been
        supplied with -machinefile=, disables the
        checks on whether all of the compute nodes
        can see the temporary directory and whether
        copies of the input files need to be made
        in order to see those files on the compute
        nodes.  Only use this option if you know
        the temporary directory and input files
        are visible to the compute nodes and you
        want to avoid the extra overhead of the
        additional checks.

    -np=n
        Sets the maximum number of processor to
        use to be n with the caveat that the
        value of the NSLOTS environment variable
        and the method for dispatching parallel
        tasks can override or modify that behavior.
        If you run the application under OpenMPI
        (-ompiexec= sets a non-empty value), the
        application will pass this option to the
        command specified by -ompiexec unless the
        NSLOTS environment variable is set to
        something other than an empty string.
        In that case, the application will ignore
        the -np= option.  If you run the
        application with a machine list
        (-machinefile= sets a non-empty value and
        -ompiexec is not used or sets an empty
        value), the application will use at most
        m processors from the machine list where
        m is either the value of the NSLOTS
        environment variable if it is set to a
        non-empty value, n if you use the -np=
        option, or the total number of processors
        in the machine list.  m must be positive
        integer.  If you do not use OpenMPI nor
        a machine list, the application will
        launch p processes on the node where the
        application runs where p is either the
        value of the NSLOTS environment variable
        if it is set to a non-empty value, n if
        you use the -np= option, or one.  p must
        be a positive integer.  The use of the
        NSLOTS environment variable is for
        compatibility with the Grid Engine
        queueing system.  That system sets NSLOTS
        to the number of processors assigned to
        a job.

    -ompiexec=file
        If file is not an empty file name, causes
        the application to assume that file
        is the equivalent of OpenMPI's mpirun or
        mpiexec and to use that command to launch
        the parallel tasks.  The version of OpenMPI
        used must be 1.3 or later.  If file is
        an empty file name or you do not use
        -ompiexec=, the parallel tasks will either
        be launched with a remote login command,
        if you specify a list of hosts to use with
        -machinefile=, or by running multiple
        processes on the host where the parallel
        script runs.

    -rshcmd=cmd
        Causes the application to use cmd to
        launch processes on the machines
        specified in the machine list.  By
        default, the command used is "ssh -x".

    -tmpdir=dir
        Causes the application to place the
        temporary files during parallel
        execution in the directory named
        dir.  When you do not explicitly set
        the temporary directory with
        -tmpdir=, the temporary files are
        placed in a directory named tmp within
        your home directory.

GPUParallel
    The GPU-accelerated calculations for tomography
    in Priism are implemented as a client-server
    architecture.  One piece of software, the GPU
    reconstruction server, works with the compatible
    NVIDIA graphics cards attached to a host.
    Other pieces of software, the clients, work with
    one or more instances of the server software to
    deliver the reconstruction to you.

    The server software is currently only available
    for x86 or x86_64 Linux systems.  It has been 
    compiled to be compatible with NVIDIA graphics
    cards that have a compute capability of 1.0 or
    higher.  To check if your hardware will be
    compatible, look at the list in

        https://developer.nvidia.com/cuda-gpus

    We have had trouble using the server software
    on graphics cards that are also being used for
    desktop display.  On most of our systems that
    we use for GPU-accelerated calculations, the
    X server is not enabled to avoid that problem.
    If you do need an X server on the system, we
    recommend that you have multiple graphics
    cards, one of which is an NVIDIA device that
    is not used for desktop display.

    The server software requires that NVIDIA's
    propietary driver be installed on the systems
    where the server software will be run.  The
    library, libcuda.so.1, that is packaged with
    the driver, must be in the search path for
    dynamically loaded libraries.  NVIDIA's
    drivers are available from

        http://www.nvidia.com/drivers

    The server software was compiled with version
    5.0.7 of CUDA and the runtime library for
    that version of CUDA, libcudart.so.5.0, is
    included with the server software.  The other
    runtime libraries that the server software
    requires are libpthread.so.0, libstdc++.so.6,
    libm.so.6, libgcc_s.so.1, and libc.so.6.

    The client software can be run on the same
    host as the server software, and that should
    not require any special configuration of the
    host beyond the requirements for the server
    software.  When run on a different host, the
    following requirements must be met:

    1) It must be possible for the client's host
       to connect to the server's host over the
       network.  The connection is through TCP
       and, by default uses port number 20248.
       It may be possible to use a different
       port number, but that requires additional
       configuration.

    2) If you have the client software start
       and stop the server software as necessary
       (that is the default behavior), you'll
       either need OpenMPI (verion 1.3 or later)
       or a command that allows you to login
       from the client's host to the server host
       without intervention from you.  That
       command has to accept the following syntax:

           command host command to execute

       One way to do this is to use ssh as the
       command (as an optimization, we use ssh -x
       so that X11 forwarding is disabled) and use
       ssh-agent to supply the password, set up
       keys so a password is not required (with
       OpenSSH this is done with an authorized_keys
       file), or use host-based authentication
       (with OpenSSH that uses .shosts or .rhosts
       (or the system-wide equivalents) and
       typically requires modifying the sshd
       configuration as well).  With rsh, you
       would use rsh as the name of the command
       and configure .rhosts (or the system-wide
       equivalent) to allow you to login without
       a password.

    3) When the client software uses multiple
       hosts for servers, it currently assumes
       that the server software can be started
       in the same way on all the hosts: that
       all the hosts are already running the
       server software or that the same rsh-style
       command and path to server executable
       can be used on all the hosts.

    The settings in the Priism user interface
    to control execution for the GPU-accelerated
    tomography calculations are listed below.
    For one common case, running the GPU
    reconstruction server with a default
    configuration on the same machine as the
    client, you should not have to change anything.

    Start/stop server
        If this toggle button is on, then the client
        software will assume that server software
        should be started and stopped as needed.
        In that case, you may need to modify the
        path to the server executable and, if the
        server runs on a different machine than
        the client, the rsh-style command to use
        to connect to the other machine and start
        or stop the server software.  When the
        toggle button is off, the client assumes
        the server software is already running.

    Server executable
        This field is only relevant if the client
        software starts and stops the server
        software.  If this field is not empty,
        the client software will use it as the
        executable or script to run on each host
        in order to start the GPU reconstruction
        server.  When this field is empty, the
        client software will use one of the
        following in order, of precedence, as
        the path to the executable (note that
        the environment variables are evaluated
        on the machine where the client runs
        and not on the machine where the server
        software will run):

        1) the value of the environment variable,
           RGEMT_EXEC, if that environment
           variable has been set

        2) the value of the environment variable,
           IVE_BASE, with "grecsrv/bin/grecsrv"
           appended, if that environment variable
           has been set

        3) /net/daa5.ucsf.edu/Volumes/home2/lehua/grecsrv/curr/bin/grecsrv

    Server port
        Sets the TCP port to use when the client
        contacts the server software on any host
        address that does not specify a port number
        (any hosts that come from OpenMPI or the
        machine list will not have a port number
        associated with them).  If the server
        software is started by the client software,
        the client software will also configure
        the server software to listen on the
        specified port.

    Memory fraction
        This only has an effect if the client starts
        the server software.  In that case, the
        client software will configure the server
        software to use the given fraction of each
        graphics card's onboard memory.  The
        fraction is limited to the range of 0.2 to
        0.8 since the server software clamps the
        value to that range.

    Busy IDs
        This only has an effect if the client starts
        the server software.  In that case, the client
        software will configure the server software 
        to ignore the given device IDs.  The device
        IDs should be non-negative integers, separated
        by spaces or commas.  If you have a graphics
        card that is also used for desktop display,
        you may need to include the ID of that
        graphics card in the list of busy IDs so that
        it is not used for computations.

    Dispatch
        The menu next to the button labeled "Dispatch"
        controls how the client determines which hosts
        are running or will run the server software.
        The first option is "simple":  the client will
        use the hosts listed in the "Server" fields.
        The second option is "MPICH-style machines
        file":  the client will look at the file
        specified by the field labeled "Machine list"
        for the hosts to use.  The third option is
        "OpenMPI mpiexec":  the client will assume
        the command given in the field labeled
        "OpenMPI mpiexec path" is equivalent to
        OpenMPI's mpiexec or mpirun and use that
        command to determine the hosts to contact.
        You would normally use that option if the
        job is submitted to a queue which dynamically
        assigns hosts and the queueing software is
        compatible with OpenMPI.

    Number of processors
        The value shown here can have two effects.
        Firstly, regardless of how the "Dispatch"
        menu has been set, it can influence the
        number of processors requested if you are
        submitting jobs to a queue.  That
        requires a customized configuration, see
        the queue topic of BatchRegion.hlp if
        you want to know more, and the selection
        of the queue to use from the "Options..."
        button in the main dialog.  Secondly,
        when the "Dispatch" menu has been set to
        "OpenMPI mpiexec" or "MPICH-style
        machines file", the client software
        either passes the specified number of
        processors to the mpiexec command or
        uses that number to control the
        processors extracted from the machines
        file.  With a custom queue configuration,
        the second effect may be disabled so that
        the requested number of processors only
        directly affects the request to the
        queueing software.

    OpenMPI mpiexec path
        If the menu next to the button labeled
        "Dispatch" is set to "OpenMPI mpiexec",
        this field specifies the command
        equivalent to OpenMPI's mpirun or
        mpiexec.  The version of OpenMPI must
        be 1.3 or higher.  If the command name is
        empty, the client will act as if in the
        "simple" dispatch mode without any host
        names in the "Server" fields.

    Machine list
        If the menu next to the button labeled
        "Dispatch" is set to "MPICH-style
        machines file", this field specifies
        the name of the file with the lists
        of hosts to use for the server
        software.  The format of the file is
        the same as described in the CPUParallel
        section on settings for parallel
        execution with multiple CPUs.  The
        client will only use unique hosts
        (judged by the host name; it does not
        test if different names resolve to
        the same address or machine) from the
        file.  The machines file format does
        not allow specification of a port
        number to use for each host so the
        server software must be configured
        to use the same port on all hosts if
        you use the machines list.  If the
        name of the machines list is empty,
        the client will act as if in the
        "simple" dispatch mode without any
        host names in the "Server" fields.

    rsh command
        This field is only relevant if the
        client software starts and stops
        the server software, the host where
        the server software will run is
        different from the host running
        the client, and the client has not
        been set to start the server software
        using OpenMPI.  In that case, the
        value of this field will be used
        as the command part of

            command host command to execute

        when starting and stopping the server
        software.  Common choices are "ssh -x",
        "ssh", or "rsh".  The given command
        must have been configured so that it
        does not prompt for a name and password
        when logging in to the remote system.

    Server
        If the menu labeled "Dispatch" is set
        to "simple", there are eight fields
        in which you can specify the host
        names where the server software is
        or will be running.  If you want the
        client software to connect over a
        specific port to the server software,
        append a colon followed by the port
        number to the host's address (if that
        port number is different than the
        default, 20248, the server software
        must have been configured to listen
        on the specified port).  Any of
        those fields which are empty will
        be ignored.  If you do not specify
        any hosts in the server fields, the
        client will look at the value of
        the RGEMT_SERVER_LIST environment
        variable and treat it as a comma-
        separated list of hosts to use.
        If that variable is not set or empty,
        the client will use the host where
        the client runs for the server
        software.

    When running jobs that use GPU acceleration
    from the command line, we recommend that
    you use rgemt_autostart (it's in the
    standard location for Priism executables)
    to start the task.  rgemt_autostart handles
    the job of determining the hosts to use
    (if you use OpenMPI or a machine list)
    and, as necessary, starting and stopping
    the server software. The syntax for
    rgemt_autostart is

        rgemt_autostart client_options autostart_options

    where client_options are the command
    line arguments and options expected by
    the client software and autostart_options
    must include -rgemt_client=client_name
    to set which client software to use and
    zero or more of the other options
    described below:

    -machinefile=filename
        Specifies that the unique hosts listed
        in the file named filename will be used
        for the GPU reconstruction server software.
        The file must be in the same format as
        a machines file for MPICH.  rgemt_autostart
        ignores this option if the -ompiexec=
        option is used with a non-empty value or
        if filename is an empty string.

    -np=n
        If you use -ompiexec= with a non-empty
        value and the NSLOTS environment variable
        is not set or is set to an empty value,
        rgemt_autostart will pass -np=n to the
        mpiexec command.  If you use -machinefile=
        with a non-empty value, then rgemt_autostart
        will use at most m hosts from the file.
        If the NSLOTS environment variable is
        not set or is set to an empty value, m
        is n; otherwise, m is the value of the
        NSLOTS environment variable.

    -ompiexec=mpiexec_path
        Causes rgemt_autostart to use mpiexec_path
        as the equivalent of OpenMPI's mpiexec or
        mpirun.  rgemt_autostart will use that
        command to determine which hosts to use
        and, if necessary, start and stop the
        server software.  If mpiexec_path is an
        empty string, rgemt_autostart ignores
        this  option.

    -rgemt_busy=i1...
        If rgemt_autostart starts the server
        software, specifies one or more device
        IDs, separated by commas, that the
        server software will not use.

    -rgemt_client=client_name
        Specifies that rgemt_autostart run
        client_name as the client software.
        The standard values for client_name
        are gpurecon, gpureproj, ra_gpu,
        or emtiar_gpu.

    -rgemt_exe=exepath
        Specifies that rgemt_autostart use
        exepath as the path to the GPU
        reconstruction server software
        on all hosts.  If you do not use
        this option, rgemt_autostart will
        use one of the following, in order
        of precedence:

        1) the value of the environment
           variable, RGEMT_EXEC, if that
           environment variable has been set
        2) the value of the environment
           variable, IVE_BASE, with
           "grecsrv/bin/grecsrv" appended,
           if that environment variable has
           been set
        3) /net/daa5.ucsf.edu/Volumes/home2/lehua/grecsrv/curr/bin/grecsrv

    -rgemt_mem=f
        If rgemt_autostart starts the server
        software, rgemt_autostart will
        instruct it to use at most a fraction,
        f, of each device's onboard memory.
        f must be a floating-point value.
        The server software will automatically
        clamp f to the range of 0.2 to 0.8.

    -rgemt_noauto
        If you include this in the options,
        rgemt_autostart will assume that the
        GPU reconstruction server software is
        already running on all of the hosts.
        Otherwise, rgemt_autostart starts the
        GPU reconstruction server software
        before starting the client software
        and terminates the GPU reconstruction
        server software after the client
        finishes.

    -rgemt_port=p
        For hosts that come from the machines
        file or are discovered with OpenMPI
        or hosts specified with -server that
        do not include a port number, causes
        the client to connect to port number,
        p, when contacting the server software.
        If rgemt_autostart starts the server
        software, rgemt_autostart will cause
        it to listen on port number, p.

    -rshcmd=cmd
        Specifies that the command, cmd, be
        used when connecting to other hosts
        to start and stop the GPU reconstruction
        server software if you are not using
        OpenMPI.  If you do not supply this
        option, rgemt_autostart will use
        "ssh -x".

    -server=name
        Specifies that name be used as a host
        for the server software.  If name has
        a colon follwed by a port number at
        the end, that port number will be used
        when contacting the server software
        on that host.  You may use multiple
        -server= options.  If you use the
        -ompiexec= or -machinefile= options
        with non-empty values, rgemt_autostart
        will ignore any -server= options you
        provide.

    Priism includes two version of the GPU
    reconstruction server software,
    grecsrv/bin/grecsrv-linux-x86 and
    grecsrv/bin/grecsrv-linux-x86_64.
    The first is for 32-bit x86 Linux.
    The second is for 64-bit x86_64 Linux.
    Priism has a script, grecsrv/bin/grecsrv,
    that chooses which executable to use
    based on the machine's architecture,
    sets the library path to include the
    version of libcudart.so.5.0 included
    with Priism, and then runs the server
    software with a current directory
    of /.  rgemt_autostart, by default,
    uses that script when starting the
    server software.

    Three attributes of the server
    software can be configured.  Those
    are the same attributes that
    rgemt_autostart can set (port number,
    memory fraction, and list of busy
    GPUs) though rgemt_autostart has
    the limitation that the attributes
    for each host in a computation are
    the same.  The server software first
    looks for a configuration file, named
    config.gpu, in the current working
    directory for the configuration
    information.  If that file is not
    present, the server software looks
    at a handful of environment variables.
    This is an example configuration file:

    ServerPort = 20248
    MemoryPercentage = 0.50
    BusyGpus = 6, 7, 8

    The first line configures the server
    software to listen for TCP connections
    on port 20248.  The second line
    configures the server software to
    use half of the onboard memory of
    each device.  Internally, the server
    will take the fraction you provide
    and clamp it to the range of 0.2
    to 0.8.  The third line configures
    the server to ignore the specified
    CUDA device IDs.  The environment
    variables that the server software
    will look at for configuration are:

    GRECSRV_PORT
        If no configuration file was
        found and the value of this
        variable is an integer in
        decimal, the reconstruction
        server will listen on the
        TCP port number given by
        that value.

    GRECSRV_MEM_PCT
        If no configuration file was
        found and the value of this
        variable is a floating-point
        value, the reconstruction
        server will use that value,
        clamped to be between 0.2
        and 0.8, as the fraction of
        each device's memory to use.

    GRECSRV_BUSY_GPUS
        If no configuration file was
        found and the value of this
        variable is a comma-separated
        list of integers, the
        reconstruction server will
        use the value as the list
        of CUDA device IDs to ignore.

    When you do not provide a
    configuration file and do not
    configure the port number in
    the environment, the
    reconstruction server will
    listen on TCP port 20248.
    When you do not provide a
    configuration file and do
    not configure the fraction
    of the device memory to use,
    the reconstruction server
    will use one half of the
    device memory.  When you do
    not provide a configuration
    file and do not specify any
    busy devices in the
    environment, the reconstruction
    server uses all the attached
    devices that CUDA detects.
